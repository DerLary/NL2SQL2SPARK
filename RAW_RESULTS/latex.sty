\UseRawInputEncoding
\documentclass{article}
\usepackage{graphicx} % Required for inserting images
% \usepackage{rotating}
\usepackage{pdflscape} % rotates the page in the PDF viewer
\usepackage{hyperref}
\usepackage{capt-of} % in preamble
\usepackage{url}
\usepackage{listings}
\usepackage{geometry}
\geometry{legalpaper, margin=0.96in}
\usepackage{amsmath}

% \renewcommand{\thesubsection}{\arabic{subsection}.}
% \renewcommand{\thesubsubsection}{\alph{subsubsection})}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{seqsplit}   % optional, helps break long tokens like 
\usepackage{longtable}
\usepackage{colortbl}
\usepackage{tabularx}




% Preamble additions:
% Highlight color for non-perfect queries:
\definecolor{badqid}{RGB}{255,235,235} % very light red

% Helper: mark a query id cell (with jaccard) as "bad" by wrapping in \cellcolor{badqid}{...}


% Preamble additions:
\usepackage[table]{xcolor}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{array}
\newcolumntype{Y}{>{\raggedright\arraybackslash}X}
%
% Highlight color for non-perfect queries:
\definecolor{badqid}{RGB}{255,235,235} % very light red

\usepackage{ltablex}
\keepXColumns

% Helper: mark a query id cell (with jaccard) as "bad" by wrapping in \cellcolor{badqid}{...}

\setlength{\parindent}{0pt}

\title{%
  Safe Distributed System Architectures\\
  \large Second Assignment}
  
\author{Lars Tissen}
\date{28. December 2025}

\newcolumntype{Y}{>{\raggedright\arraybackslash}X}

\begin{document}

\maketitle

\section{Implementation} \label{sec:impl}
In this section, I briefly describe my implementation of the automated benchmark pipeline and the functions that had to be completed. The full implementation is available as a fork of the original repository at \href{https://github.com/DerLary/NL2SQL2SPARK}{this link}. 

The main entry point for running the automated pipeline is the file \href{https://github.com/DerLary/NL2SQL2SPARK/blob/main/query_workflow.py}{query\_workflow.py}. Depending on the provided command-line arguments, different parts of the pipeline can be executed. If only a query ID is given using \textit{--id}, the benchmark is executed only for this specific query ID, using the default model (Google) if no other provider is specified. I also partially implemented support for Cloudflare, which is discussed later in this section. Additionally, by passing the argument \textit{--hil-query [QUERY]}, a user-defined golden query can be provided against which the inferred query is evaluated (used in Section~\ref{sec:hitl}); otherwise, the golden query from the test database is used.

A central part of the implementation is the function \textit{translate\_sqlite\_to\_spark}, which we had to implement. In principle, \textit{sqlglot.parse\_one(...)} is used to translate a SQLite query into a Spark SQL query. However, I observed that the translated queries were sometimes not executable by Spark. For example, expressions such as \texttt{SUM((gender = F))} were produced, which are invalid in Spark SQL. Therefore, I had to manually adjust such expressions to make them executable. 

If a translated query executes successfully and returns a result, the Jaccard index is computed using the function \textit{jaccard\_index}. During testing, I observed that Spark returns all values as strings, even if the underlying table columns are of integer type. To avoid incorrect comparisons due to type mismatches, I additionally implemented \textit{jaccard\_index\_new}, which explicitly converts all values to strings before comparison. Both functions compute the Jaccard index by transforming the rows of the result tables into sets and then comparing the result sets of the golden and inferred queries.

When the argument \textit{--run-pipeline} is provided, the full automated benchmark pipeline is executed. The function \textit{benchmark\_queries} starts the benchmark by randomly selecting a configured number of simple, moderate, and challenging query IDs. Each query is then benchmarked for a specified number of iterations. For each individual query, the function \textit{benchmark\_query} is called. Further details of this process can be found directly in the code.

As part of the benchmarking process, the function \textit{load\_tables} is used to load the database tables into Spark. During this step, I encountered an issue where some tables (for example, in query ID 134) contained a column named \texttt{order}, which is a reserved keyword in Spark SQL. To handle this, I added quotation marks around such column names when loading the tables. The original table names, however, were not changed when registering them in Spark, as can be seen in the implementation.

If no model provider is explicitly specified, the default provider \texttt{google} is used via the function \textit{get\_llm}. I also implemented support for a \texttt{cloudflare} provider. However, I observed that the Cloudflare agent does not return responses in the format required by LangChain: instead of returning only the action and parameters, it includes additional text. As a result, the agent was not able to correctly retrieve information about the database tables. I attempted to work around this issue by manually injecting table information into the prompt (see \href{https://github.com/DerLary/NL2SQL2SPARK/blob/2c037f5185edbdfa5f1fc1cb72cb1bf39834a473/src/spark_nl.py#L264}{here}), but this did not lead to satisfactory results. Therefore, all final experiments were conducted using the Google provider only.

The implementations of the remaining required functions, such as \textit{get\_spark\_session} and \textit{run\_sparksql\_query}, are relatively straightforward.

After the benchmark pipeline has completed, the results can be aggregated by passing the argument \textit{--only-aggregate}. This step uses the aggregation functions implemented in \href{https://github.com/DerLary/NL2SQL2SPARK/blob/main/src/aggregation.py}{aggregation.py}. During benchmarking, the results of each run are stored separately for each query ID and iteration in individual files and subfolders. The aggregation process first sorts the result files (if necessary), then creates an aggregated result file for each query ID, and finally produces a global aggregated file for the entire benchmark run. In addition, multiple benchmark runs can be aggregated into a single combined result file.

When the argument \textit{--plotting-json} is provided, plots are generated based on the given JSON file. These plots are presented in Section~\ref{sec:plots}. To reduce computation time, intermediate results (for aggregation and plotting) are cached and stored in files. The plotting functions only recompute values if the corresponding output files do not yet exist or if a configuration flag explicitly forces recomputation.



\section{Evaluation}\label{sec:plots}
For the evaluation, I measured the performance of the system using the model \texttt{gemini-2.5-flash}. I randomly selected ten queries for each difficulty level (simple, moderate, and challenging), resulting in a total of 30 queries (see Section~\ref{sec:impl}). To obtain more reliable results, each query was evaluated ten times. The results are presented in the following sections.

The exact match score (EA), computed using \textit{eval\_exact\_match}, as well as the translation, Spark execution, and total execution times, are calculated as defined in the original repository. The Jaccard index is computed by transforming all result values to strings, as described in Section~\ref{sec:impl}. Red numbers next to the query IDs indicate how many iterations resulted in an execution error for the inferred query. The boxplots visualize the results of all ten iterations for each query ID. Standard boxplots are used: the box represents the interquartile range (IQR), the whiskers extend to $1.5 \times$ IQR, the orange line indicates the median, and isolated points denote outliers beyond the whiskers.

The evaluation results are presented separately for each difficulty level. For each difficulty, a figure shows the aggregated evaluation metrics, followed by a table containing detailed information. Each table lists the Jaccard index (J) and exact match accuracy (EA) for each query ID, together with the corresponding natural language (NL), golden, and inferred (Spark) queries. Whenever the Jaccard index is not equal to 1, the corresponding cell is highlighted in light red and an error explanation is provided, describing why the inferred query differs from the golden query. In most cases, the model infers identical or very similar queries across all ten iterations. When multiple different inferred queries occur, they are explicitly shown (see, for example, query ID 168 in Table~\ref{tab:examples_moderate_jaccard}). For the "challenging" queries there exist queries that yield different Spark queries for each iteration. For those queries I do not explicitly state all inferred queries but only one/some representative query.

The results for the "simple" queries are shown in Figure \ref{fig:simple} and Table \ref{tab:examples_simple_jaccard}, the "moderate" queries in Figure \ref{fig:moderate} and Table \ref{tab:examples_moderate_jaccard}, and the "challenging" queries in Figure \ref{fig:challenging} and Table \ref{tab:examples_challenging_jaccard}.

\subsection{"Simple" Queries}
Figure~\ref{fig:simple} shows that the median Jaccard index is equal to 1 for five out of the ten simple queries. As detailed in Table~\ref{tab:examples_simple_jaccard}, most cases where the Jaccard index is below 1 are due to the inferred query returning additional columns compared to the golden query (e.g., query IDs 30, 983, and 1385) or misinterpreting specific column values (e.g., query IDs 593 and 1532). For query ID 1532, both the Jaccard and exact match values are reported as ``--'' because the inferred query causes a division-by-zero error during execution.
The exact match metric checks whether the inferred and golden queries match exactly in terms of tables, joins, column names, and structure. Since identical results can be obtained using structurally different queries, cases occur where the Jaccard index differs from the exact match score (for example, query ID 30). Inspecting the queries in Table~\ref{tab:examples_simple_jaccard} shows that the inferred queries often use different naming conventions or query constructions.

The total execution time is dominated by the translation step, which takes on average 14.24 seconds. Spark execution contributes only a small fraction of the total time, with an average of 0.14 seconds. The shortest execution times were observed for query IDs 134 and 776, which both correspond to relatively short and simple queries (see Table \ref{tab:examples_simple_jaccard}). 

\subsection{"Moderate" Queries}
Figure~\ref{fig:moderate} presents the results for the moderate queries. Similar to the simple case, five out of ten queries achieve a Jaccard index of 1, while three queries achieve an exact match score of 1. The reasons for lower Jaccard indices are more diverse than for the simple queries (see Table~\ref{tab:examples_moderate_jaccard}). For query IDs 168 and 892, the model introduces incorrect joins; for query ID 48, required filters are omitted or incorrectly applied; for query ID 150, the model misinterprets the natural language query and uses a distinct count instead of a simple count; for query ID 1212, not all required filters are applied and additional columns are returned; and for query ID 557, the inferred query is semantically correct but likely yields different results due to rounding or precision effects.

Despite the increased difficulty of the queries, the execution times remain similar to those of the simple queries. The translation step again dominates the total time, with an average of 14.89 seconds (approximately 0.7 seconds more than for the simple queries), while the average Spark execution time remains at around 0.14 seconds. 

\subsection{"Challenging" Queries}
Figure~\ref{fig:challenging} shows the results for the challenging queries. Only one query (query ID 1429) achieves a Jaccard index of 1. For the remaining queries, the Jaccard index is 0.5 on average for query ID 1169, 0.1 for query ID 1223, and 0 for all others. Similarly, the exact match score is 0 for most queries, except for query IDs 1223 and 1429. The reasons for the low Jaccard indices are varied and are described in detail in Table~\ref{tab:examples_challenging_jaccard}. Overall, the model appears to struggle with translating complex natural language queries into semantically equivalent Spark SQL queries.

The average total execution time increases slightly for the challenging queries, mainly due to longer translation times. The average translation time increases to 16.86 seconds (approximately two seconds more than for the moderate queries), while the average Spark execution time increases slightly to 0.151 seconds.

In summary, the model generally succeeds in returning the requested values for simple queries, although it often returns additional information, which reduces the Jaccard index while still including the correct results. For moderate queries, the performance is mixed, with errors arising from a variety of issues beyond simply returning additional columns. For challenging queries, the model fails to return correct results in most cases, indicating that additional prompt engineering or guidance would be required to improve performance.

In the following section, I select several queries that produced incorrect results and refine their natural language descriptions to investigate whether the model can be guided to infer the correct queries.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Exercise_3/benchmark_simple.png}
    \caption{Evaluation results of the "simple" queries. Boxplots show the results per query_id for 10 iterations of the benchmark. Red numbers next to the query_id indicate the number of errors yielded when the inferred queries were executed.}
    \label{fig:simple}
\end{figure}



\begin{landscape}


% =========================
% SIMPLE
% =========================
% \begin{table}[ht]
\begin{ltablex}
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.25}
\begin{tabularx}{1.97\textwidth}{|c|Y|}
\hline
\textbf{ID} & \textbf{Query: Simple} \\ \hline
% 30 (jaccard 0.0) -> colored + error explanation row
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{30}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} Which cities have the top 5 lowest enrollment number for students in grades 1 through 12? K-12 refers to students in grades 1 through 12. \\ \cline{2-2}
& \textbf{Gold:} SELECT T2.City FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode GROUP BY T2.City ORDER BY SUM(T1.`Enrollment (K-12)`) ASC LIMIT 5 \\ \cline{2-2}
& \textbf{Spark:} SELECT s.City, SUM(f.`Enrollment (K-12)`) AS TotalEnrollment FROM frpm AS f JOIN schools AS s ON f.CDSCode = s.CDSCode GROUP BY s.City ORDER BY TotalEnrollment ASC LIMIT 5 \\ \cline{2-2}
& \textbf{Error explanation:} The inferred query outputs more than asked for (adds an extra aggregated column): instead of "SELECT T2.City" have "SELECT T2.City, SUM(..) and not only cities. \\ \hline
% 134 (jaccard 0.7) -> colored + error explanation row
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{134}\\\footnotesize J=0.7\\ EA=0.0}} &
\textbf{NL:} In the branch where the largest number of crimes were committed in 1996, how many accounts were opened? A16 stands for no. of committed crimes 1996 \\ \cline{2-2}
& \textbf{Gold:} SELECT COUNT(T2.account\_id) FROM district AS T1 INNER JOIN account AS T2 ON T1.district\_id = T2.district\_id GROUP BY T1.A16 ORDER BY T1.A16 DESC LIMIT 1 \\ \cline{2-2}
& \textbf{Spark:} SELECT COUNT(account\_id) FROM account WHERE district\_id = (SELECT district\_id FROM district ORDER BY `A16` DESC LIMIT 1) \\ \cline{2-2}
& \textbf{Error explanation:} In 3/10 cases the AI model seems to return early by outputting  only the condition of  "district\_id" as in "Spark" above but does not plug in in the greater "SELECT" statement. \\ \hline
% 593 (jaccard 0.0) -> colored + error explanation row
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{593}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} How many users from New York have a teacher and supporter badge? \"Supporter\" and \"Teachers\" are both Name of badge; 'New York' is the Location; user refers to UserId\\ \cline{2-2}
& \textbf{Gold:} SELECT COUNT(DISTINCT T1.Id) FROM badges AS T1 INNER JOIN users AS T2 ON T1.UserId = T2.Id WHERE T1.Name IN ('Supporter', 'Teacher') AND T2.Location = 'New York' \\ \cline{2-2}
& \textbf{Spark:} SELECT COUNT(DISTINCT `u`.`Id`) FROM `users` AS `u` JOIN `badges` AS `b\_supporter` ON `u`.`Id` = `b\_supporter`.`UserId` JOIN `badges` AS `b\_teacher` ON `u`.`Id` = `b\_teacher`.`UserId` WHERE `u`.`Location` = 'New York' AND `b\_supporter`.`Name` = 'Supporter' AND `b\_teacher`.`Name` = 'Teachers'; \\ \cline{2-2}
& \textbf{Error explanation:} (1) The golden query understands "teacher and supporter badge" as OR while the model understands it as AND (two self-joins). (2) Although the NL says "Teachers" it uses "Teacher" while the model sticks with "Teachers" likely yielding zero matches. \\ \hline
% 776 (jaccard 1.0) -> no explanation row
\multirow{3}{*}{\shortstack{\textbf{776}\\\footnotesize J=1.0\\ EA=1.0}} &
\textbf{NL:} Provide the hero name and race of Charles Chandler. hero name refers to superhero\_name; Charles Chandler is the full name of superhero; \\ \cline{2-2}
& \textbf{Gold:} SELECT T1.superhero\_name, T2.race FROM superhero AS T1 INNER JOIN race AS T2 ON T1.race\_id = T2.id WHERE T1.full\_name = 'Charles Chandler' \\ \cline{2-2}
& \textbf{Spark:} SELECT T1.superhero\_name, T2.race FROM superhero AS T1 JOIN race AS T2 ON T1.race\_id = T2.id WHERE T1.full\_name = 'Charles Chandler' \\ \hline
% 778 (jaccard 1.0)
\multirow{3}{*}{\shortstack{\textbf{778}\\\footnotesize J=1.0\\ EA=0.7}} &
\textbf{NL:} Provide superheroes' names who have the adaptation power. adaptation power refers to power\_name = 'Adaptation'; \\ \cline{2-2}
& \textbf{Gold:} SELECT T1.superhero\_name FROM superhero AS T1 INNER JOIN hero\_power AS T2 ON T1.id = T2.hero\_id INNER JOIN superpower AS T3 ON T2.power\_id = T3.id WHERE T3.power\_name = 'Adaptation' \\ \cline{2-2}
& \textbf{Spark:} SELECT s.superhero\_name FROM superhero AS s JOIN hero\_power AS hp ON s.id = hp.hero\_id JOIN superpower AS sp ON hp.power\_id = sp.id WHERE sp.power\_name = 'Adaptation' \\ \hline
% 983 (jaccard 0.0) -> colored + error explanation row
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{983}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} Which of the Italian constructor got the highest point to date? Give its introduction website? introduction website refers to url; Italian is a nationality \\ \cline{2-2}
& \textbf{Gold:} SELECT T1.url FROM constructors AS T1 INNER JOIN constructorStandings AS T2 ON T1.constructorId = T2.constructorId WHERE T1.nationality = 'Italian' ORDER BY T2.points DESC LIMIT 1 \\ \cline{2-2}
& \textbf{Spark:} SELECT T1.name, T1.url, SUM(T2.points) AS total\_points FROM constructors AS T1 JOIN constructorstandings AS T2 ON T1.constructorId = T2.constructorId WHERE T1.nationality = 'Italian' GROUP BY T1.name, T1.url ORDER BY total\_points DESC LIMIT 1 \\ \cline{2-2}
& \textbf{Error explanation:} The inferred query aggregates (\texttt{SUM(points)}) and returns extra columns (name, url, total\_points) instead of only the single \texttt{url} which was asked for. \\ \hline
% 1326 (jaccard 1.0)
\multirow{3}{*}{\shortstack{\textbf{1326}\\\footnotesize J=1.0\\ EA=1.0}} &
\textbf{NL:} How many members of the Student\_Club have majored Environmental Engineering? `Environmental Engineering' is the major name \\ \cline{2-2}
& \textbf{Gold:} SELECT COUNT(T1.member\_id) FROM member AS T1 INNER JOIN major AS T2 ON T1.link\_to\_major = T2.major\_id WHERE T2.major\_name = 'Environmental Engineering' \\ \cline{2-2}
& \textbf{Spark:} SELECT COUNT(T1.member\_id) FROM member AS T1 JOIN major AS T2 ON T1.link\_to\_major = T2.major\_id WHERE T2.major\_name = 'Environmental Engineering' \\ \hline
% 1374 (jaccard 1.0)
\multirow{3}{*}{\shortstack{\textbf{1374}\\\footnotesize J=1.0\\ EA=1.0}} &
\textbf{NL:} How many events did the member with the phone number ``954-555-6240'' attend? \\ \cline{2-2}
& \textbf{Gold:} SELECT COUNT(T2.link\_to\_event) FROM member AS T1 INNER JOIN attendance AS T2 ON T1.member\_id = T2.link\_to\_member WHERE T1.phone = '954-555-6240' \\ \cline{2-2}
& \textbf{Spark:} SELECT COUNT(T2.link\_to\_event) FROM member AS T1 INNER JOIN attendance AS T2 ON T1.member\_id = T2.link\_to\_member WHERE T1.phone = '954-555-6240'; \\ \hline
% 1385 (jaccard 0.5) -> colored + error explanation row
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{1385}\\\footnotesize J=0.5\\ EA=0.2}} &
\textbf{NL:} Which student was able to generate income more than \$40? name of students means the full name; full name refers to first\_name, last\_name; generate income more than \$50 refers to income.amount > 40 \\ \cline{2-2}
& \textbf{Gold:} SELECT T1.first\_name, T1.last\_name FROM member AS T1 INNER JOIN income AS T2 ON T1.member\_id = T2.link\_to\_member WHERE T2.amount > 40 \\ \cline{2-2}
& \textbf{Spark:} SELECT CONCAT(m.first\_name, ' ', m.last\_name) AS full\_name FROM member AS m JOIN income AS i ON m.member\_id = i.link\_to\_member WHERE i.amount > 40 \\ \cline{2-2}
& \textbf{Error explanation:} The model changes the output format: golden returns two columns (first\_name, last\_name) while inferred returns one concatenated column (\texttt{full\_name}), so result comparison by rows/columns differs. \\ \hline
% 1532 (jaccard None) -> colored + error explanation row
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{1532}\\\footnotesize J=--\\ EA=--}} &
\textbf{NL:} Which country had the gas station that sold the most expensive product id No.2 for one unit? \\ \cline{2-2}
& \textbf{Gold:} SELECT T2.Country FROM transactions\_1k AS T1 INNER JOIN gasstations AS T2 ON T1.GasStationID = T2.GasStationID WHERE T1.ProductID = 2 ORDER BY T1.Price DESC LIMIT 1 \\ \cline{2-2}
& \textbf{Spark:} SELECT T2.Country FROM transactions\_1k AS T1 INNER JOIN gasstations AS T2 ON T1.GasStationID = T2.GasStationID WHERE T1.ProductID = 2 ORDER BY T1.Price / T1.Amount DESC LIMIT 1 \\ \cline{2-2}
& \textbf{Error explanation:} The golden query understands price per unit as \texttt{Price} while the model tries to compute it by dividing \texttt{Price/Amount}. \texttt{Amount} seems to be 0 for some cases so there is a "divide by zero" error and no valid queries. \\ \hline
\end{tabularx}
\caption{Table showing the \textbf{Simple} queries. For each query, the NL query, golden query and the inferred Spark SQL query are given together with the average Jaccard index. Queries that yield a Jaccard index below 1.0 are marked in red and an explanation is given why the Jaccard index is below 1.}
\label{tab:examples_simple_jaccard}
% \end{table}
\end{ltablex}
\newpage
\end{landscape}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Exercise_3/benchmark_moderate.png}
    \caption{Evaluation results of the "moderate" queries. Boxplots show the results per query_id for 10 iterations of the benchmark. Red numbers next to the query_id indicate the number of errors yielded when the inferred queries were executed.}
    \label{fig:moderate}
\end{figure}




\begin{landscape}
% =========================
% MODERATE
% =========================
% \begin{table}[ht]
\begin{ltablex}
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.25}
\begin{tabularx}{1.97\textwidth}{|c|Y|}
\hline
\textbf{ID} & \textbf{Query: Moderate} \\ \hline

% 48 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{48}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} What is the ratio of merged Unified School District schools in Orange County to merged Elementary School District schools? Elementary School District refers to DOC = 52; Unified School District refers to DOC = 54. \\ \cline{2-2}
& \textbf{Gold:} SELECT CAST(SUM(CASE WHEN DOC = 54 THEN 1 ELSE 0 END) AS FLOAT) / SUM(CASE WHEN DOC = 52 THEN 1 ELSE 0 END) FROM schools WHERE StatusType = 'Merged' AND County = 'Orange' \\ \cline{2-2}
& \textbf{Spark:} SELECT CAST(SUM(CASE WHEN DOC = '54' THEN 1 ELSE 0 END) AS REAL) / SUM(CASE WHEN DOC = '52' THEN 1 ELSE 0 END) FROM schools WHERE County = 'Orange County' \\ \cline{2-2}
& \textbf{Error explanation:} The inferred query does not apply the required filter (\texttt{StatusType='Merged'} and filters for \texttt{County='Orange'}) instead of \texttt{'Orange County'}. \\ \hline

% 150 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{150}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} How many accounts in North Bohemia has made a transaction with the partner's bank being AB? A3 contains the region names; North Bohemia is a region. \\ \cline{2-2}
& \textbf{Gold:} SELECT COUNT(T2.account\_id) FROM district AS T1 INNER JOIN account AS T2 ON T1.district\_id = T2.district\_id INNER JOIN trans AS T3 ON T2.account\_id = T3.account\_id WHERE T3.bank = 'AB' AND T1.A3 = 'north Bohemia' \\ \cline{2-2}
& \textbf{Spark:} SELECT COUNT(DISTINCT T1.account\_id) FROM account AS T1 JOIN district AS T2 ON T1.district\_id = T2.district\_id JOIN trans AS T3 ON T1.account\_id = T3.account\_id WHERE T2.A3 = 'north Bohemia' AND T3.bank = 'AB' \\ \cline{2-2}
& \textbf{Error explanation:} The model switches from \texttt{COUNT(account\_id)} to \texttt{COUNT(DISTINCT account\_id)}, changing the question from ``how many accounts (with transactions)'' to ``how many unique accounts'', which can differ when multiple matching transactions exist. \\ \hline

% 168 (0.3)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{168}\\\footnotesize J=0.3\\ EA=0.0}} &
\textbf{NL:} What percentage of clients who opened their accounts in the district with an average salary of over 10000 are women? Female refers to gender = 'F'; Woman and female are closed; Average salary can be found in A11\\ \cline{2-2}
& \textbf{Gold:} SELECT CAST(SUM(CASE WHEN T2.gender = 'F' THEN 1 ELSE 0 END) AS FLOAT) * 100 / COUNT(T2.client\_id) FROM district AS T1 INNER JOIN client AS T2 ON T1.district\_id = T2.district\_id WHERE T1.A11 > 10000 \\ \cline{2-2}
& \textbf{Spark:} 

\textbf{correct:}   "SELECT SUM(CASE WHEN T2.gender = 'F' THEN 1 ELSE 0 END) * 100.0 / COUNT(T2.client\_id) FROM district AS T1 INNER JOIN client AS T2 ON T1.district_id = T2.district_id WHERE T1.A11 > 10000;",


\textbf{wrong:} SELECT CAST(SUM(CASE WHEN T3.gender = 'F' THEN 1 ELSE 0 END) AS DOUBLE) * 100 / COUNT(T3.client\_id) FROM district AS T1 INNER JOIN account AS T2 ON T1.district\_id = T2.district\_id INNER JOIN client AS T3 ON T2.district\_id = T3.district\_id WHERE T1.A11 > 10000; \\
\cline{2-2}
& \textbf{Error explanation:} In 7/10 cases the model introduces an unnecessary join (marked above as "wrong"), which can duplicate clients (or filter them) depending on account multiplicity, changing numerator/denominator of the percentage. \\ \hline

% 544 (1.0)
\multirow{3}{*}{\shortstack{\textbf{544}\\\footnotesize J=1.0\\ EA=0.9}} &
\textbf{NL:} What is the display name of the user who last edited the post \"Examples for teaching: Correlation does not mean causation\"? \"Examples for teaching: Correlation does not mean causation\" is the Title of post; user who last edited refers to LastEditorUserId \\ \cline{2-2}
& \textbf{Gold:} SELECT T2.DisplayName FROM posts AS T1 INNER JOIN users AS T2 ON T1.LastEditorUserId = T2.Id WHERE T1.Title = 'Examples for teaching: Correlation does not mean causation' \\ \cline{2-2}
& \textbf{Spark:} SELECT T2.DisplayName FROM posts AS T1 INNER JOIN users AS T2 ON T1.LastEditorUserId = T2.Id WHERE T1.Title = 'Examples for teaching: Correlation does not mean causation' \\ \hline

% 557 (0.2)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{557}\\\footnotesize J=0.2\\ EA=0.0}} &
\textbf{NL:} Among the posts with a score of over 5, what is the percentage of them being owned by an elder user? score of over 5 refers to Score > 5; elder user refers to Age > 65; percentage = Divide (Count(Id where Age>65), Count(Id)) * 100\\ \cline{2-2}
& \textbf{Gold:} SELECT CAST(SUM(IF(T2.Age > 65, 1, 0)) AS FLOAT) * 100 / COUNT(T1.Id) FROM posts AS T1 INNER JOIN users AS T2 ON T1.OwnerUserId = T2.Id WHERE T1.Score > 5 \\ \cline{2-2}
& \textbf{Spark:} SELECT (SUM(CASE WHEN T2.`Age` > 65 THEN 1 ELSE 0 END) * 100.0) / COUNT(T1.`Id`) FROM `posts` AS T1 INNER JOIN `users` AS T2 ON T1.`OwnerUserId` = T2.`Id` WHERE T1.`Score` > 5; \\ \cline{2-2}
& \textbf{Error explanation:} Semantics are correct but the difference stem from slightly different values (probably because or preciosion/rounding). For example: golden_query: "0.16899404073645824" and AI query_result: "0.16899404073646". \\ \hline

% 845 (1.0)
\multirow{3}{*}{\shortstack{\textbf{845}\\\footnotesize J=1.0\\ EA=0.0}} &
\textbf{NL:} List the power of superheroes with height greater than 80\% of the average height of all superheroes. power of superheroes refers to power_name; height greater than 80\% of the average height of all superheroes = height\_cm > MULTIPLY(AVG(height\_cm), 0.8); \\ \cline{2-2}
& \textbf{Gold:} SELECT T3.power\_name FROM superhero AS T1 INNER JOIN hero\_power AS T2 ON T1.id = T2.hero\_id INNER JOIN superpower AS T3 ON T2.power\_id = T3.id WHERE T1.height\_cm * 100 > (SELECT AVG(height\_cm) FROM superhero) * 80 \\ \cline{2-2}
& \textbf{Spark:} SELECT T3.power\_name FROM superhero AS T1 INNER JOIN hero\_power AS T2 ON T1.id = T2.hero\_id INNER JOIN superpower AS T3 ON T2.power\_id = T3.id WHERE T1.height\_cm > (SELECT AVG(height\_cm) * 0.8 FROM superhero) \\ \hline

% 879 (0.8)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{879}\\\footnotesize J=0.8\\ EA=0.8}} &
\textbf{NL:} For the driver who set the fastest lap speed, what is his nationality? the fastest lap speed refers to (MAX) fastestLapSpeed; \\ \cline{2-2}
& \textbf{Gold:} SELECT T1.nationality FROM drivers AS T1 INNER JOIN results AS T2 ON T2.driverId = T1.driverId ORDER BY T2.fastestLapSpeed DESC LIMIT 1 \\ \cline{2-2}
& \textbf{Spark:} 

\textbf{correct:} SELECT T1.nationality FROM drivers AS T1 JOIN results AS T2 ON T1.driverId = T2.driverId ORDER BY T2.fastestLapSpeed DESC LIMIT 1

\textbf{wrong:} SELECT T2.nationality FROM results AS T1 INNER JOIN drivers AS T2 ON T1.driverId = T2.driverId ORDER BY CAST(T1.fastestLapSpeed AS DOUBLE) DESC LIMIT 1\\ \cline{2-2}
& \textbf{Error explanation:} In 2/10 queries the model queries the "wrong" query given above in which it applies an additional join and thereby yields wrong results.


applied a different join. \\ \hline

% 892 (0.9)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{892}\\\footnotesize J=0.9\\ EA=0.0}} &
\textbf{NL:} State the driver with the most points scored. Find his full name with that points. the most points scored refers to max(points); Full name of the driver refers to drivers.forename and drivers.surname; \\ \cline{2-2}
& \textbf{Gold:} SELECT T3.forename, T3.surname, T2.points FROM races AS T1 INNER JOIN driverStandings AS T2 ON T2.raceId = T1.raceId INNER JOIN drivers AS T3 ON T3.driverId = T2.driverId ORDER BY T2.points DESC LIMIT 1 \\ \cline{2-2}
& \textbf{Spark:} SELECT d.forename, d.surname, ds.points FROM drivers AS d JOIN driverstandings AS ds ON d.driverId = ds.driverId ORDER BY ds.points DESC LIMIT 1; \\ \cline{2-2}
& \textbf{Error explanation:} In 1/10 queries the model applied a different join. \\ \hline

% 895 (1.0)
\multirow{3}{*}{\shortstack{\textbf{895}\\\footnotesize J=1.0\\ EA=1.0}} &
\textbf{NL:} What is the average lap time for Lewis Hamilton in the 2009 Malaysian Grand Prix? average lap time = AVG(milliseconds); 'Lewis Hamilton' refers to the full name of the driver; Full name of the driver refers to drivers.forename and drivers.surname; 'Malaysian Grand Prix' refers to races.name = 'Malaysian Grand Prix' \\ \cline{2-2}
& \textbf{Gold:} SELECT AVG(T2.milliseconds) FROM races AS T1 INNER JOIN lapTimes AS T2 ON T2.raceId = T1.raceId INNER JOIN drivers AS T3 ON T3.driverId = T2.driverId WHERE T3.forename = 'Lewis' AND T3.surname = 'Hamilton' AND T1.year = 2009 AND T1.name = 'Malaysian Grand Prix' \\ \cline{2-2}
& \textbf{Spark:} SELECT AVG(lt.milliseconds) FROM laptimes lt JOIN drivers d ON lt.driverId = d.driverId JOIN races r ON lt.raceId = r.raceId WHERE d.forename = 'Lewis' AND d.surname = 'Hamilton' AND r.name = 'Malaysian Grand Prix' AND r.year = 2009 \\ \hline

% 1212 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{1212}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} For patients with alkaliphophatase (ALP) within normal range, were they treated as inpatient or outpatient? alkaliphophatase (ALP) within normal range refers to ALP < 300; inpatient refers to admission = '+'; outpatient refers to admission = '-'\\ \cline{2-2}
& \textbf{Gold:} SELECT T1.Admission FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.ALP < 300 \\ \cline{2-2}
& \textbf{Spark:} SELECT T1.Admission, COUNT(T1.Admission) FROM patient AS T1 JOIN laboratory AS T2 ON T1.ID = T2.ID WHERE T2.ALP < 300 GROUP BY T1.Admission; \\ \cline{2-2}
& \textbf{Error explanation:} The model does not requests for "Admission" but also asks for the counts, so returns more than asked for. \\ \hline

\end{tabularx}
\caption{Table showing the \textbf{Moderate} queries. For each query, the NL query, golden query and the inferred Spark SQL query are given together with the average Jaccard index. Queries that yield a Jaccard index below 1.0 are marked in red and an explanation is given why the Jaccard index is below 1.}
\label{tab:examples_moderate_jaccard}
% \end{table}
\end{ltablex}
\newpage
\end{landscape}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{Exercise_3/benchmark_challenging.png}
    \caption{Evaluation results of the "challenging" queries. Boxplots show the results per query_id for 10 iterations of the benchmark. Red numbers next to the query_id indicate the number of errors yielded when the inferred queries were executed.}
    \label{fig:challenging}
\end{figure}

\begin{landscape}
% =========================
% CHALLENGING
% =========================
% \begin{table}[ht]
\begin{ltablex}
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.25}
\begin{tabularx}{1.97\textwidth}{|c|Y|}
\hline
\textbf{ID} & \textbf{Query: Challenging} \\ \hline

% 28 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{28}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} Consider the average difference between K-12 enrollment and 15-17 enrollment of schools that are locally funded, list the names and DOC type of schools which has a difference above this average. Difference between K-12 enrollment and 15-17 enrollment can be computed by `Enrollment (K-12)` - `Enrollment (Ages 5-17)` \\ \cline{2-2}
& \textbf{Gold:} SELECT T2.School, T2.DOC FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.FundingType = 'Locally funded' AND (T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)`) > (SELECT AVG(T3.`Enrollment (K-12)` - T3.`Enrollment (Ages 5-17)`) FROM frpm AS T3 INNER JOIN schools AS T4 ON T3.CDSCode = T4.CDSCode WHERE T4.FundingType = 'Locally funded') \\ \cline{2-2}
& \textbf{Spark:} SELECT AVG(T2.`Enrollment (K-12)` - T2.`Enrollment (Ages 5-17)`) FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode WHERE T1.FundingType = 'Local' \\ \cline{2-2}
& \textbf{Error explanation:} The model returns only the average and also changes the funding filter value. It does not output the requested school names and DOC types above the average threshold. \\ \hline

% 83 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{83}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} Of the schools that offers a magnet program serving a grade span of Kindergarten to 8th grade, how many offers Multiple Provision Types? List the number of cities that offers a Kindergarten to 8th grade span and indicate how many schools are there serving such grade span for each city. Kindergarten to 8th grade refers to K-8; 'Offers a magnet program' means Magnet = 1; Multiple Provision Types refers to `NSLP Provision Status` = 'Multiple Provision Types'. \\ \cline{2-2}
& \textbf{Gold:} SELECT T2.City, COUNT(T2.CDSCode) FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.Magnet = 1 AND T2.GSoffered = 'K-8' AND T1.`NSLP Provision Status` = 'Multiple Provision Types' GROUP BY T2.City \\ \cline{2-2}
& \textbf{Spark:} 

\textbf{wrong1:} SELECT DISTINCT GSserved FROM schools LIMIT 10

\textbf{wrong2:} SELECT COUNT(DISTINCT s.School) FROM schools AS s JOIN frpm AS f ON s.CDSCode = f.CDSCode WHERE s.Magnet = 1 AND f.`Low Grade` = 'K' AND f.`High Grade` = '8' AND f.`NSLP Provision Status` = 'Multiple Provision Types'
\\ \cline{2-2}


& \textbf{Error explanation:} The model infers two different (wrong) statements as shown above. The first ignores almost all constraints and the second returns only a count while the query asks for the city name and count (with wrong filters). \\ \hline

% 125 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{125}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} For loans contracts which are still running where client are in debt, list the district of the and the state the percentage unemployment rate increment from year 1995 to 1996. Unemployment increment rate in percentage = [(unemployment rate 2016 - unemployment rate 2015) / unemployment rate 2015] * 100; unemployment rate 2015 appears in the A12; unemployment rate 2016 appears in the A13; Loan contracts which are still running where client are in debt can be presented as status = 'D' \\ \cline{2-2}
& \textbf{Gold:} SELECT CAST((T3.A13 - T3.A12) AS FLOAT) * 100 / T3.A12 FROM loan AS T1 INNER JOIN account AS T2 ON T1.account\_id = T2.account\_id INNER JOIN district AS T3 ON T2.district\_id = T3.district\_id WHERE T1.status = 'D' \\ \cline{2-2}
& \textbf{Spark:} SELECT T3.A3 AS district\_name, T3.A2 AS state\_region, ((CAST(T3.A13 AS DOUBLE) - T3.A12) / T3.A12) * 100 AS unemployment\_rate\_increment FROM loan AS T1 JOIN account AS T2 ON T1.account\_id = T2.account\_id JOIN district AS T3 ON T2.district\_id = T3.district\_id WHERE T1.status = 'D' \\ \cline{2-2}
& \textbf{Error explanation:} The computed percentage is correct but the model additionally asks for the columns "district" and "state" which was not asked for. \\ \hline

% 219 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{219}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} What is the percentage of carcinogenic molecules in triple type bonds? label = '+' mean molecules are carcinogenic; triple bond refers to bond_type = '#'; percentage = DIVIDE(SUM(bond_type = '#') * 100, COUNT(bond_id)) as percent where label = '+' \\ \cline{2-2}
& \textbf{Gold:} SELECT CAST(COUNT(DISTINCT CASE WHEN T2.label = '+' THEN T2.molecule\_id ELSE NULL END) AS FLOAT) * 100 / COUNT(DISTINCT T2.molecule\_id) FROM atom AS T1 INNER JOIN molecule AS T2 ON T1.molecule\_id = T2.molecule\_id INNER JOIN bond AS T3 ON T2.molecule\_id = T3.molecule\_id WHERE T3.bond\_type = '\#' \\ \cline{2-2}
& \textbf{Spark:} SELECT (SUM(CASE WHEN T2.bond\_type = '\#' THEN 1 ELSE 0 END) * 100.0) / COUNT(T2.bond\_id) AS percentage\_triple\_bonds FROM molecule AS T1 INNER JOIN bond AS T2 ON T1.molecule\_id = T2.molecule\_id WHERE T1.label = '+' \\ \cline{2-2}
& \textbf{Error explanation:} The model flips the condition: it filters carcinogenic molecules first and then computes percentage of triple bonds, instead of computing percentage of carcinogenic molecules \emph{within} the triple-bond set. It also counts bonds, not distinct molecules. \\ \hline
% 944 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{944}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} How much faster in percentage is the champion than the driver who finished the race last in the 2008 Australian Grand Prix? how much faster in percentage = divide(subtract(incremental time, champion time), last_driver time) * 100; last driver finished time = incremental time + champion time; only champion's finished time is represented by 'HH:MM:SS.mmm'; finished the game refers to time is not null. \\ \cline{2-2}
& \textbf{Gold:} WITH time\_in\_seconds AS (SELECT T1.positionOrder, CASE WHEN T1.positionOrder = 1 THEN (CAST(SUBSTRING(T1.time, 1, 1) AS FLOAT) * 3600) + (CAST(SUBSTRING(T1.time, 3, 2) AS FLOAT) * 60) + CAST(SUBSTRING(T1.time, 6) AS FLOAT) ELSE CAST(SUBSTRING(T1.time, 2) AS FLOAT) END AS time\_seconds FROM results AS T1 INNER JOIN races AS T2 ON T1.raceId = T2.raceId WHERE T2.name = 'Australian Grand Prix' AND NOT T1.time IS NULL AND T2.year = 2008), champion\_time AS (SELECT time\_seconds FROM time\_in\_seconds WHERE positionOrder = 1), last\_driver\_incremental AS (SELECT time\_seconds FROM time\_in\_seconds WHERE positionOrder = (SELECT MAX(positionOrder) FROM time\_in\_seconds)) SELECT (CAST((SELECT time\_seconds FROM last\_driver\_incremental) AS FLOAT) * 100) / (SELECT time\_seconds + (SELECT time\_seconds FROM last\_driver\_incremental) FROM champion\_time) \\ \cline{2-2}
& \textbf{Spark:} 

\textbf{query1: } WITH RaceResults AS (SELECT T2.milliseconds, T2.position FROM races AS T1 INNER JOIN results AS T2 ON T1.raceId = T2.raceId WHERE T1.year = 2008 AND T1.name = 'Australian Grand Prix' AND T2.time IS NOT NULL) SELECT ((SELECT milliseconds FROM RaceResults ORDER BY position DESC LIMIT 1) - (SELECT milliseconds FROM RaceResults WHERE position = 1)) * 100.0 / (SELECT milliseconds FROM RaceResults ORDER BY position DESC LIMIT 1) AS percentage\_faster; 

\textbf{query2: } SELECT raceId FROM races WHERE year = 2008 AND name = 'Australian Grand Prix'

\textbf{query3: } SELECT (CAST(MAX(CASE WHEN T2.time IS NOT NULL THEN T2.milliseconds END) AS DOUBLE) - CAST(MAX(CASE WHEN T2.position = 1 THEN T2.milliseconds END) AS DOUBLE)) / CAST(MAX(CASE WHEN T2.time IS NOT NULL THEN T2.milliseconds END) AS DOUBLE) * 100 FROM races AS T1 INNER JOIN results AS T2 ON T1.raceId = T2.raceId WHERE T1.year = 2008 AND T1.name = 'Australian Grand Prix'


\\ \cline{2-2}
& \textbf{Error explanation:} The model outputs three different queries as shown above. The first and third queries seem correct but both return slightly different values (probably because of precision/rounding): "0.31555732173208506" from the gold query vs "0.315557322860" from query1 and "0.31555732286030097" from query2.

The second query selects raceIds and ignores the rest.\\ \hline

% 1169 (0.5)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{1169}\\\footnotesize J=0.5\\ EA=0.0}} &
\textbf{NL:} What is the ratio of male to female patients among all those with abnormal uric acid counts? male refers to SEX = 'M'; female refers to SEX = 'F'; abnormal uric acid refers to UA < = '8.0' where SEX = 'M', UA < = '6.5' where SEX = 'F'; calculation = DIVIDE(SUM(UA <= '8.0' and SEX = 'M'), SUM(UA <= '6.5 and SEX = 'F'))\\ \cline{2-2}
& \textbf{Gold:} SELECT CAST(SUM(CASE WHEN T2.UA <= 8.0 AND T1.SEX = 'M' THEN 1 ELSE 0 END) AS FLOAT) / SUM(CASE WHEN T2.UA <= 6.5 AND T1.SEX = 'F' THEN 1 ELSE 0 END) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID \\ \cline{2-2}
& \textbf{Spark:} 

\textbf{correct:} SELECT CAST(SUM(CASE WHEN P.SEX = 'M' AND L.UA <= 8.0 THEN 1 ELSE 0 END) AS DOUBLE) / CAST(SUM(CASE WHEN P.SEX = 'F' AND L.UA <= 6.5 THEN 1 ELSE 0 END) AS DOUBLE) FROM patient AS P JOIN laboratory AS L ON P.ID = L.ID;

\textbf{wrong:} SELECT SUM(CASE WHEN p.`SEX` = 'M' AND l.`UA` <= 8.0 THEN 1 ELSE 0 END) AS male\_abnormal\_ua\_count, SUM(CASE WHEN p.`SEX` = 'F' AND l.`UA` <= 6.5 THEN 1 ELSE 0 END) AS female\_abnormal\_ua_count FROM `patient` AS p JOIN `laboratory` AS l ON p.`ID` = l.`ID`;


\\ \cline{2-2}
& \textbf{Error explanation:} In 5/10 cases the model uses the above shown wrong query. In this wrong query, the model returns the both abnormal counts but not the ratio of them. 

The SQL is logically equivalent; partial mismatch usually indicates comparison issues (float formatting, row canonicalization, or schema/column-name differences) rather than a true semantic error. \\ \hline

% 1223 (0.1)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{1223}\\\footnotesize J=0.1\\ EA=0.1}} &
\textbf{NL:} Are there more male patients with creatinine not within the normal range than female? True or False? creatinine (CRE) not within the normal range refers to CRE > = 1.5; male refers to Sex = 'M'; female refers to Sex = 'F'; calculation = (SUM(SEX = 'M') > SUM(SEX = 'F')) where CRE > = 1.5 \\ \cline{2-2}
& \textbf{Gold:} SELECT CASE WHEN SUM(CASE WHEN T1.SEX = 'M' THEN 1 ELSE 0 END) > SUM(CASE WHEN T1.SEX = 'F' THEN 1 ELSE 0 END) THEN 'True' ELSE 'False' END FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID WHERE T2.CRE >= 1.5 \\ \cline{2-2}
& \textbf{Spark:} 

\textbf{correct: } SELECT CASE WHEN SUM(CASE WHEN T1.SEX = 'M' THEN 1 ELSE 0 END) > SUM(CASE WHEN T1.SEX = 'F' THEN 1 ELSE 0 END) THEN 'True' ELSE 'False' END AS more\_male\_than\_female FROM patient AS T1 INNER JOIN laboratory AS T2 ON T1.ID = T2.ID WHERE T2.CRE >= 1.5; 

\textbf{wrong: } SELECT SUM(CASE WHEN P.SEX = 'M' THEN 1 ELSE 0 END) AS MaleCount, SUM(CASE WHEN P.SEX = 'F' THEN 1 ELSE 0 END) AS FemaleCount FROM patient AS P JOIN laboratory AS L ON P.ID = L.ID WHERE L.CRE >= 1.5;

\\ \cline{2-2}
& \textbf{Error explanation:} In 9/10 cases the model formulates the above shown wrong query in which the query does not return True/False but the counts of male and female patients.. \\ \hline

% 1241 (0.1)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{1241}\\\footnotesize J=0.0\\ EA=0.0}} &
\textbf{NL:} For patients with abnormal platelet level, state the number of patients with lower than normal range. How is it compare to the number of patients with higher than normal range? abnormal platelet level refers to PLT <= 100 or PLT >= 400; platelet level lower than normal range refers to PLT < 100; calculation = SUBTRACT(SUM(PLT < 100), SUM(PLT > 400)); platelet level higher than normal range refers to PLT > 400; \\ \cline{2-2}
& \textbf{Gold:} SELECT SUM(CASE WHEN T2.PLT <= 100 THEN 1 ELSE 0 END) - SUM(CASE WHEN T2.PLT >= 400 THEN 1 ELSE 0 END) FROM Patient AS T1 INNER JOIN Laboratory AS T2 ON T1.ID = T2.ID
 \\ \cline{2-2}
& \textbf{Spark:} 
SELECT SUM(CASE WHEN PLT < 100 THEN 1 ELSE 0 END) AS lower_than_normal, SUM(CASE WHEN PLT > 400 THEN 1 ELSE 0 END) AS higher_than_normal, SUM(CASE WHEN PLT < 100 THEN 1 ELSE 0 END) - SUM(CASE WHEN PLT > 400 THEN 1 ELSE 0 END) AS difference FROM laboratory WHERE PLT <= 100 OR PLT >= 400\\ \cline{2-2}
& \textbf{Error explanation:} The inferred query does not only return a single value (which the golden query does but in my opinion the NL query asks for more) and uses < 100 and > 400 instead of <= and >=. \\ \hline

% 1429 (1.0)
\multirow{3}{*}{\shortstack{\textbf{1429}\\\footnotesize J=1.0\\ EA=0.9}} &
\textbf{NL:} What is the name of the social event that was attended by the vice president of the Student_Club located at 900 E. Washington St.? name of social event refers to event_name where type = 'Social'; 'Vice President' is position; located at refers to location
 \\ \cline{2-2}
& \textbf{Gold:} SELECT T2.event\_name FROM attendance AS T1 INNER JOIN event AS T2 ON T2.event\_id = T1.link\_to\_event INNER JOIN member AS T3 ON T1.link\_to\_member = T3.member\_id WHERE T3.position = 'Vice President' AND T2.location = '900 E. Washington St.' AND T2.type = 'Social'
\\ \cline{2-2}
& \textbf{Spark:} SELECT T1.event\_name FROM event AS T1 INNER JOIN attendance AS T2 ON T1.event\_id = T2.link\_to\_event INNER JOIN member AS T3 ON T2.link\_to\_member = T3.member\_id WHERE T1.type = 'Social' AND T3.position = 'Vice President' AND T1.location = '900 E. Washington St.'
 \\ \hline

% 1482 (0.0)
\multirow{4}{*}{\cellcolor{badqid}\shortstack{\textbf{1482}\\\footnotesize J=0.0 \\ EA=0.0}} &
\textbf{NL:} Which of the three segments\u2014SME, LAM and KAM\u2014has the biggest and lowest percentage increases in consumption paid in EUR between 2012 and 2013? Increase or Decrease = consumption for 2013 - consumption for 2012; Percentage of Increase = (Increase or Decrease / consumption for 2013) * 100\%; The first 4 strings of the Date values in the yearmonth table can represent year\\ \cline{2-2}
& \textbf{Gold:} SELECT CAST((SUM(IF(T1.Segment = 'SME' AND T2.Date LIKE '2013\%', T2.Consumption, 0)) - SUM(IF(T1.Segment = 'SME' AND T2.Date LIKE '2012\%', T2.Consumption, 0))) AS FLOAT) * 100 / SUM(IF(T1.Segment = 'SME' AND T2.Date LIKE '2012\%', T2.Consumption, 0)), CAST(SUM(IF(T1.Segment = 'LAM' AND T2.Date LIKE '2013\%', T2.Consumption, 0)) - SUM(IF(T1.Segment = 'LAM' AND T2.Date LIKE '2012\%', T2.Consumption, 0)) AS FLOAT) * 100 / SUM(IF(T1.Segment = 'LAM' AND T2.Date LIKE '2012\%', T2.Consumption, 0)), CAST(SUM(IF(T1.Segment = 'KAM' AND T2.Date LIKE '2013\%', T2.Consumption, 0)) - SUM(IF(T1.Segment = 'KAM' AND T2.Date LIKE '2012\%', T2.Consumption, 0)) AS FLOAT) * 100 / SUM(IF(T1.Segment = 'KAM' AND T2.Date LIKE '2012\%', T2.Consumption, 0)) FROM customers AS T1 INNER JOIN yearmonth AS T2 ON T1.CustomerID = T2.CustomerID \\ \cline{2-2}
& \textbf{Spark:} 

\textbf{error1: } WITH ConsumptionByYear AS (SELECT c.`Segment`, YEAR(t.`Date`) AS `Year`, SUM(t.`Price`) AS `TotalConsumption` FROM `customers` c JOIN `transactions\_1k` t ON c.`CustomerID` = t.`CustomerID` WHERE c.`Currency` = 'EUR' AND YEAR(t.`Date`) IN (2012, 2013) GROUP BY c.`Segment`, YEAR(t.`Date`)), SegmentConsumption AS (SELECT `Segment`, SUM(CASE WHEN `Year` = 2012 THEN `TotalConsumption` ELSE 0 END) AS `Consumption2012`, SUM(CASE WHEN `Year` = 2013 THEN `TotalConsumption` ELSE 0 END) AS `Consumption2013` FROM ConsumptionByYear GROUP BY `Segment`) SELECT `Segment`, `Consumption2012`, `Consumption2013`, (`Consumption2013` - `Consumption2012`) AS `IncreaseDecrease`, ( CAST((`Consumption2013` - `Consumption2012`) AS DOUBLE) / NULLIF(`Consumption2013`, 0) ) * 100 AS `PercentageIncrease` FROM SegmentConsumption ORDER BY `PercentageIncrease` DESC;

\textbf{wrong1: } SELECT c.Segment, SUBSTRING(y.Date, 1, 4) AS Year, SUM(y.Consumption) AS TotalConsumption FROM customers c JOIN yearmonth y ON c.CustomerID = y.CustomerID WHERE c.Currency = 'EUR' AND SUBSTRING(y.Date, 1, 4) IN ('2012', '2013') GROUP BY c.Segment, SUBSTRING(y.Date, 1, 4) ORDER BY c.Segment, Year

\\ \cline{2-2}
& \textbf{Error explanation:} In 5/10 cases the model's queries try to get the year of the date (error1) which returns an error. In the other cases, the queries also return the segments names (wrong1) but the golden query does not (in my opinion, the NL query asks for the segment's name). Also, in some cases the year is returned and in general the computations are wrong. 

\\ \hline

\end{tabularx}
\caption{Table showing the \textbf{Challenging} queries. For each query, the NL query, golden query and the inferred Spark SQL query are given together with the average Jaccard index. Queries that yield a Jaccard index below 1.0 are marked in red and an explanation is given why the Jaccard index is below 1.}
\label{tab:examples_challenging_jaccard}
% \end{table}
\end{ltablex}
\end{landscape}


\section{Human in the Loop (HITL)}\label{sec:hitl}

\begin{landscape}


\begin{table}[ht]
\centering
\small
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}

\begin{tabularx}{1.95\textwidth}{|c|c|X|}
\hline
\textbf{Query ID} & \textbf{Iteration} & \textbf{Content} \\ \hline

% 30
\multirow{7}{*}{\textbf{1}} &  &


\textbf{Original NL:} Which cities have the top 5 lowest enrollment number for students in grades 1 through 12? K-12 refers to students in grades 1 through 12. \\ \cline{2-3}
& & \textbf{Gold Query:} SELECT T2.City FROM frpm AS T1 INNER JOIN schools AS T2 ON T1.CDSCode = T2.CDSCode GROUP BY T2.City ORDER BY SUM(T1.`Enrollment (K-12)`) ASC LIMIT 5 \\ \cline{2-3}
& & \textbf{Wrong Spark:} SELECT s.City, SUM(f.`Enrollment (K-12)`) AS TotalEnrollment FROM frpm AS f JOIN schools AS s ON f.CDSCode = s.CDSCode GROUP BY s.City ORDER BY TotalEnrollment ASC LIMIT 5 \\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 1}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} D 
\\ \cline{3-3}
&  & 
\textbf{Spark:} E 
\\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 2}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} F 
\\ \cline{3-3}
&  & 
\textbf{Spark:} G 
\\ \hline

% 134
\multirow{7}{*}{\textbf{1}} &  &
\textbf{Original NL:} A \\ \cline{2-3}
&  & \textbf{Gold Query:} B \\ \cline{2-3}
&  & \textbf{Wrong Spark:} C \\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 1}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} D 
\\ \cline{3-3}
&  & 
\textbf{Spark:} E 
\\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 2}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} F 
\\ \cline{3-3}
&  & 
\textbf{Spark:} G 
\\ \hline

% 593
\multirow{7}{*}{\textbf{1}} &  &
\textbf{Original NL:} A \\ \cline{2-3}
&  & \textbf{Gold Query:} B \\ \cline{2-3}
&  & \textbf{Wrong Spark:} C \\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 1}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} D 
\\ \cline{3-3}
&  & 
\textbf{Spark:} E 
\\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 2}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} F 
\\ \cline{3-3}
&  & 
\textbf{Spark:} G 
\\ \hline

% 983
\multirow{7}{*}{\textbf{1}} &  &
\textbf{Original NL:} A \\ \cline{2-3}
&  & \textbf{Gold Query:} B \\ \cline{2-3}
&  & \textbf{Wrong Spark:} C \\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 1}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} D 
\\ \cline{3-3}
&  & 
\textbf{Spark:} E 
\\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 2}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} F 
\\ \cline{3-3}
&  & 
\textbf{Spark:} G 
\\ \hline

% 1385
\multirow{7}{*}{\textbf{1}} &  &
\textbf{Original NL:} A \\ \cline{2-3}
&  & \textbf{Gold Query:} B \\ \cline{2-3}
&  & \textbf{Wrong Spark:} C \\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 1}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} D 
\\ \cline{3-3}
&  & 
\textbf{Spark:} E 
\\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 2}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} F 
\\ \cline{3-3}
&  & 
\textbf{Spark:} G 
\\ \hline

% 1532
\multirow{7}{*}{\textbf{1}} &  &
\textbf{Original NL:} A \\ \cline{2-3}
&  & \textbf{Gold Query:} B \\ \cline{2-3}
&  & \textbf{Wrong Spark:} C \\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 1}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} D 
\\ \cline{3-3}
&  & 
\textbf{Spark:} E 
\\ \cline{2-3}
& \multirow{2}{*}{\shortstack{\textbf{Iter 2}\\\footnotesize J=0.0 \\ EA=0.0}} & 
\textbf{NL:} F 
\\ \cline{3-3}
&  & 
\textbf{Spark:} G 
\\ \hline

\end{tabularx}
\caption{Minimal table with iteration blocks spanning NL and SQL rows.}
\label{tab:min_debug_iter}
\end{table}


\end{landscape}


\end{document}

